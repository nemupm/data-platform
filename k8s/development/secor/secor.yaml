# $ https://github.com/pinterest/secor/tree/master/deploys/helm/secor
# $ helm template secor ./deploys/helm/secor > secor.yaml
---
# Source: secor/templates/secor-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: secor-config
  labels:
    app: secor-config
    chart: secor-0.1.0
    release: secor
    heritage: Helm
data:
  log4j.dev.properties: |
    # log4j logging dev configuration.
  
    # root logger.
    log4j.rootLogger=DEBUG, CONSOLE, ROLLINGFILE
  
    log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender
    log4j.appender.CONSOLE.Threshold=INFO
    log4j.appender.CONSOLE.Target=System.err
    log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
    log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] (%C:%L) %-5p %m%n
  
    log4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender
    log4j.appender.ROLLINGFILE.Threshold=DEBUG
    log4j.appender.ROLLINGFILE.File=/tmp/secor_dev/logs/secor.log
    # keep log files up to 1G
    log4j.appender.ROLLINGFILE.MaxFileSize=20MB
    log4j.appender.ROLLINGFILE.MaxBackupIndex=50
    log4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayout
    log4j.appender.ROLLINGFILE.layout.ConversionPattern=%d{ISO8601} [%t] (%C:%L) %-5p %m%n
  log4j.docker-warn.properties: |
    # log4j logging configuration.
  
    # root logger.
    log4j.rootLogger=WARN, CONSOLE
  
    log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender
    log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
    log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] (%c) %-5p %m%n
  log4j.docker.properties: |
    # log4j logging configuration.
  
    # root logger.
    log4j.rootLogger=INFO, CONSOLE
  
    log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender
    log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
    log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] (%c) %-5p %m%n
  log4j.kubernetes-prod.properties: |
    # log4j logging configuration.
  
    # root logger.
    log4j.rootLogger=INFO, CONSOLE
  
    log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender
    log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
    log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] (%c) %-5p %m%n
  
    # Dont want to be bothered about hadoop saying "Im using compression" every time we upload a file
    log4j.logger.org.apache.hadoop.io.compress.CodecPool = WARN
    log4j.logger.com.pinterest.secor.uploader.GsUploadManager = DEBUG
  log4j.prod.properties: |
    # log4j logging configuration.
  
    # root logger.
    log4j.rootLogger=DEBUG, CONSOLE, ROLLINGFILE
  
    log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender
    log4j.appender.CONSOLE.Threshold=WARN
    log4j.appender.CONSOLE.Target=System.err
    log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
    log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [%t] (%C:%L) %-5p %m%n
  
    log4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender
    log4j.appender.ROLLINGFILE.Threshold=INFO
    log4j.appender.ROLLINGFILE.File=/mnt/secor_data/logs/secor-${secor_group}.log
    # keep log files up to 1G
    log4j.appender.ROLLINGFILE.MaxFileSize=20MB
    log4j.appender.ROLLINGFILE.MaxBackupIndex=50
    log4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayout
    log4j.appender.ROLLINGFILE.layout.ConversionPattern=%d{ISO8601} [%t] (%C:%L) %-5p %m%n
  secor.kubernetes-prod.backup.properties: |+
    include=secor.kubernetes-prod.properties
  
    # Name of the Google cloud storage bucket where log files are stored.
    secor.gs.bucket=logs-secor-backup-prod
  
    # Google cloud storage path where files are stored within the bucket.
    secor.gs.path=raw_logs
  
    # Name of the Kafka consumer group.
    secor.kafka.group=secor_backup
  
    # Parser class that extracts partitions from consumed messages.
    secor.message.parser.class=com.pinterest.secor.parser.OffsetMessageParser
  
    # S3 path where sequence files are stored.
    secor.s3.path=raw_logs/secor_backup
  
    # Swift path where sequence files are stored.
    secor.swift.path=
  
    # Local path where sequence files are stored before they are uploaded to s3.
    secor.local.path=/mnt/secor_data/message_logs/backup
  
    # Port of the Ostrich server.
    ostrich.port=9999
  
  secor.kubernetes-prod.partition.properties: |2
  
    include=secor.kubernetes-prod.properties
  
    # Name of the Google cloud storage bucket where log files are stored.
    secor.gs.bucket=logs-secor-prod
  
    # Google cloud storage path where files are stored within the bucket.
    secor.gs.path=data
  
    # Name of the Kafka consumer group.
    secor.kafka.group=secor_partition
  
    # Parser class that extracts s3 partitions from consumed messages.
    secor.message.parser.class=com.pinterest.secor.parser.JsonMessageParser
  
    # Swift path where sequence files are stored.
    secor.swift.path=
  
    # Local path where sequence files are stored before they are uploaded to s3.
    secor.local.path=/mnt/secor_data/message_logs/partition
  
    # Port of the Ostrich server.
    ostrich.port=9998
  secor.kubernetes-prod.properties: |+
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
  
    ############
    # MUST SET #
    ############
  
    # Regular expression matching names of consumed topics.
    secor.kafka.topic_filter=log.event
    secor.kafka.topic_blacklist=
  
    # Choose what to fill according to the service you are using
    # in the choice option you can fill S3, GS, Swift or Azure
    cloud.service=GS
  
    # AWS authentication credentials.
    # Leave empty if using IAM role-based authentication with s3a filesystem.
    aws.access.key=
    aws.secret.key=
    # Session token only required if using temporary S3 access keys
    aws.session.token=
    aws.role=
  
    # Optional Proxy Setting. Set to true to enable proxy
    # Only applicable to S3UploadManager
    aws.proxy.isEnabled=false
    aws.proxy.http.host=
    aws.proxy.http.port=
  
    kafka.seed.broker.host=kafka-kafka.default
  
    ################
    # END MUST SET #
    ################
  
  
    # AWS region or endpoint. region should be a known region name (eg.
    # us-east-1). endpoint should be a known S3 endpoint url. If neither
    # are specified, then the default region (us-east-1) is used. If both
    # are specified then endpoint is used.
    #
    # Only apply if the the S3UploadManager is used - see
    # secor.upload.manager.class.
    #
    # http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region
    aws.region=
    aws.endpoint=
  
    # Toggle the AWS S3 client between virtual host style access and path style
    # access. See http://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html
    aws.client.pathstyleaccess=false
  
    ###########################
    # START AWS S3 ENCRYPTION #
    ###########################
  
    # AWS specify type of server-side encryption, if any
    # set to S3 to enable S3-managed encryption
    # set to KMS to enable AWS KMS-managed encryption (see aws.sse.kms.key)
    # set to customer to enable customer-managed encryption (see aws.sse.customer.key)
    # set empty to disable encryption
    aws.sse.type=
  
    # Key to use for S3 server-side encryption, base64-encoded
    # Note: requires aws.sse.type to be set to customer to be used
    aws.sse.customer.key=
  
    # KMS Key to use for S3 server-side encryption, base64-encoded
    # Leave empty to use default generated key
    # Note: requires aws.sse.type to be set to  KMS to be used
    aws.sse.kms.key=
  
    #########################
    # END AWS S3 ENCRYPTION #
    #########################
  
    # Hadoop filesystem to use. Choices are s3n or s3a.
    # See https://wiki.apache.org/hadoop/AmazonS3 for details.
    secor.s3.filesystem=s3n
  
    # Swift config, MUST configure if cloud.service=Swift
  
    # Swift Login Details:
    swift.use.get.auth=true
    swift.auth.url=
    swift.tenant=
    swift.username=
    swift.port=8080
    swift.public=true
  
    # only needed if "swift.use.get.auth" = false
    swift.password=
  
    # only needed if "swift.use.get.auth" = true
    swift.api.key=
  
    # GS config, MUST configure if gcloud.service=GS
  
    # Use direct uploads
    # WARNING: disables resumable uploads, files are uploaded in a single request
    # This may help prevent IOException: insufficient data written,
    # see https://github.com/pinterest/secor/issues/177
    # https://cloud.google.com/storage/docs/json_api/v1/how-tos/upload
    secor.gs.upload.direct=false
  
    # Limit the amonut of API calls towards GCS
    # This usually helps against exceptinos of: com.google.cloud.storage.StorageException: Error writing request body to server
    # Cold/Low traffic buckets need to be "warmed" up most likely before you can increase this value.. this is untested.
    # Sadly the client library doesn't say which http response code happens, ie 429 / 5xx etc.
    # Also see https://cloud.google.com/storage/docs/request-rate
    secor.gs.tasks.ratelimit.pr.second=2
  
    # Old behavior of number of threads was 256 , but running this on kubernetes we experience we wanted less threads here,
    # to work well with ratelimit above.
    secor.gs.threadpool.fixed.size=32
  
  
    # Application credentials configuration file
    # https://developers.google.com/identity/protocols/application-default-credentials
    # It can be empty when secor running in Google Cloud VMs with proper scopes
    secor.gs.credentials.path=
  
    # Zookeeper config.
    zookeeper.session.timeout.ms=3000
    zookeeper.sync.time.ms=200
  
    # Zookeeper path (chroot) under which secor data will be placed.
    secor.zookeeper.path=/
  
    # Impacts how frequently the upload logic is triggered if no messages are delivered.
    kafka.consumer.timeout.ms=10000
  
    # Where consumer should read from if no committed offset in zookeeper.
    #   "smallest" -> read from earliest offset
    #   "largest"  -> read from latest offset
    # Always use "smallest" unless you know what you're doing and are willing to risk
    # data loss for new topics or topics whose number of partitions has changed.
    # See the kafka docs for "auto.offset.reset".
    kafka.consumer.auto.offset.reset=smallest
  
    # Choose between range and roundrobin partition assignment strategy for kafka
    # high level consumers. Check PartitionAssignor.scala in kafa 821 module for
    # the differences between the two.
    # In kafka 811, only range strategy is supported.
    kafka.partition.assignment.strategy=range
  
    # Max number of retries during rebalance.
    kafka.rebalance.max.retries=
  
    # Rebalance backoff.
    kafka.rebalance.backoff.ms=
  
    # Kafka consumer receive buffer size (socket.receive.buffer.bytes)
    kafka.socket.receive.buffer.bytes=
  
    # Kafka fetch max size (fetch.message.max.bytes)
    kafka.fetch.message.max.bytes=
  
    # Kafka fetch min bytes (fetch.fetch.min.bytes)
    kafka.fetch.min.bytes=
  
    # Kafka fetch max wait ms (fetch.max.wait.ms)
    kafka.fetch.wait.max.ms=
  
    # Port of the broker serving topic partition metadata.
    kafka.seed.broker.port=9092
  
    # Zookeeper path at which kafka is registered. In Zookeeper parlance, this is referred
    # to as the chroot.
    kafka.zookeeper.path=/
  
    #URL of a Confluent Schema Registry: https://docs.confluent.io/current/schema-registry/docs/index.html
    #Only acquired used for decoding Avro messages
    schema.registry.url=
  
    # Store offset in zookeeper and kafka consumer topic.
    # Only used if kafka.offsets.storage is set to "kafka"
    # http://kafka.apache.org/documentation.html#oldconsumerconfigs
    # Possible values: true or false
    kafka.dual.commit.enabled=false
  
    # Storage offset.
    # Possible values: "zookeeper" to read offset from zookeeper or "kafka" to read offset from kafka consumer topic
    kafka.offsets.storage=zookeeper
  
    # Parameter which tells whether to extract Kafka message timestamp. This value is to be chose in case of 0.10.x kafka brokers.
    # Default value is false. Also specify `kafka.message.timestamp.className` as `com.pinterest.secor.timestamp.Kafka10MessageTimestamp`,
    # in case you are enabling this parameter as `true`.
    kafka.useTimestamp=false
  
    # Classname for the timestamp field you want to use. Default is `com.pinterest.secor.timestamp.Kafka10MessageTimestamp`
    # for 0.10 build profile. Basically, it will be `Kafka8MessageTimestamp` for 0.8 kafka and `Kafka10MessageTimestamp`
    # for 0.10 kafka. Fully classified names are `com.pinterest.secor.timestamp.Kafka8MessageTimestamp` and
    # `com.pinterest.secor.timestamp.Kafka10MessageTimestamp`.
    kafka.message.timestamp.className=com.pinterest.secor.timestamp.Kafka10MessageTimestamp
  
    # Classname for the message iterator you want to use. The message iterator determines what kind of consumer
    # secor will use to communicate with kafka. com.pinterest.secor.reader.LegacyKafkaMessageIterator uses
    # the old kafka consumer written scala. Its not recommended to use the legacy iterator with kafka version >= 1.0 since it
    # does not support the new broker protocols. You may face significant peformance degration on your brokers if you use it
    kafka.message.iterator.className=com.pinterest.secor.reader.LegacyKafkaMessageIterator
  
    # Secor generation is a version that should be incremented during non-backwards-compatible
    # Secor releases. Generation number is one of the components of generated log file names.
    # Generation number makes sure that outputs of different Secor versions are isolated.
    secor.generation=1
  
    # Number of consumer threads per Secor process.
    secor.consumer.threads=6
  
    # Consumption rate limit enforced at the process level (not a consumer-thread level).
    # Increasing this value can give higher throughput and performance from kafka!
    secor.messages.per.second=400000
  
    # Used by the "backup" consumer group only.
    # Number of continuous message offsets that constitute a single offset= partition on s3.
    # Example:
    #   if set to 10,
    #     messages with offsets 0 to 9 will be written to s3 path s3n://.../offset=0/...
    #     messages with offsets 10 to 19 will be written to s3 path s3n://.../offset=10/...
    #     ...
    secor.offsets.per.partition=10000000
    secor.offsets.prefix=offset=
    # How long does it take for secor to forget a topic partition. Applies to stats generation only.
    secor.topic_partition.forget.seconds=600
  
    # Setting the partitioner to use hourly partition
    # By default, the partitioner will do daily partition, so the data will be
    # written into
    #       s3n://.../topic/dt=2015-07-07/
    # If this parameter is set to true, the data will be written into
    #       s3n://.../topic/dt=2015-07-07/hr=02
    # The hour folder ranges from 00 to 23
    partitioner.granularity.hour=false
    partitioner.granularity.minute=false
  
    partitioner.granularity.date.prefix=dt=
    partitioner.granularity.hour.prefix=hr=
    partitioner.granularity.minute.prefix=min=
  
    partitioner.granularity.date.format=yyyy-MM-dd
    partitioner.granularity.hour.format=HH
    partitioner.granularity.minute.format=mm
  
    # how many seconds should the finalizer wait to finalize a partition
    partitioner.finalizer.delay.seconds=3600
  
    # During partition finalization, the finalizer will start from the last
    # time partition (e.g. dt=2015-07-17) and traverse backwards for n
    # partition periods (e.g. dt=2015-07-16, dt=2015-07-15 ...)
    # This parameter controls how many partition periods to traverse back
    # The default is 10
    # secor.finalizer.lookback.periods=10
  
    # If greater than 0, upon startup Secor will clean up directories and files under secor.local.path
    # that are older than this value.
    secor.local.log.delete.age.hours=1
  
    # Secor comes with a tool that adds Hive partitions for finalized topics. Currently, we support
    # only Hive clusters accessible through Qubole. The token gives access to the Qubole API.
    # It is available at https://api.qubole.com/users/edit
    qubole.api.token=
  
    # hive tables are generally named after the topics. For instance if the topic
    # is request_log the hive table is also called request_log. If you want this
    # to be pinlog_request_log you can set this config to "pinlog_". This affects
    # all topics.
    hive.table.prefix=
  
    # You can also name your hive table directly if your hive table doesn't
    # follow the pattern of <hive.table.prefix><kafka topic>
    # E.g.  hive.table.name.topic1=table1 to indicate that hive table for
    # kafka topic <topic1> will be named <table1>
  
    # Secor can export stats such as consumption lag (in seconds and offsets) per topic partition.
    # Leave empty to disable this functionality.
    tsdb.hostport=
  
    # Regex of topics that are not exported to TSDB.
    monitoring.blacklist.topics=
  
    # Prefix of exported stats.
    monitoring.prefix=secor
  
    # Monitoring interval.
    # Set to 0 to disable - the progress monitor will run once and exit.
    monitoring.interval.seconds=60
  
    # Secor can export stats to statsd such as consumption lag (in seconds and offsets) per topic partition.
    # Leave empty to disable this functionality.
    statsd.hostport=localhost:9125
  
    # Thrift protocol class. It applies to timestamp extractor below and parquet output for thrift messages.
    # TBinaryProtocol by default
    secor.thrift.protocol.class=
  
    # Thrift message class. It applies to parquet output.
    # If all Kafka topics transfer the same thrift message type, set secor.thrift.message.class.*=<thrift class name>
    secor.thrift.message.class.*=
  
    # If true, the consumer group will be the initial prefix of all
    # exported metrics, before `monitoring.prefix` (if set).
    #
    # Setting to false and use monitoring.prefix can lead to nice paths.
    # For example,
    #   secor.kafka.group = secor_hr_partition
    #   monitoring.prefix = secor.hr
    #   statsd.prefixWithConsumerGroup = false
    #   => secor.hr.lag.offsets.<topic>.<partition>
    #
    #   secor.kafka.group = secor_hr_partition
    #   monitoring.prefix = secor
    #   statsd.prefixWithConsumerGroup = true
    #   => secor_hr_partition.secor.lag.offsets.<topic>.<partition>
    statsd.prefixWithConsumerGroup=true
  
    # Name of field that contains timestamp for JSON, MessagePack, or Thrift message parser. (1405970352123)
    message.timestamp.name=timestamp
  
    # Separator for defining message.timestamp.name in a nested structure. E.g.
    # {"meta_data": {"created": "1405911096123", "last_modified": "1405912096123"}, "data": "test"}
    # message.timestamp.name=meta_data.created
    # message.timestamp.name.separator=.
    message.timestamp.name.separator=
  
    # Field ID of the field that contains timestamp for Thrift message parser.
    # N.B. setting this past 1 will come with a performance penalty
    message.timestamp.id=1
  
    # Data type of the timestamp field for thrift message parser.
    # Supports i64 and i32.
    message.timestamp.type=i64
  
    # Name of field that contains a timestamp, as a date Format, for JSON. (2014-08-07, Jul 23 02:16:57 2005, etc...)
    # Should be used when there is no timestamp in a Long format. Also ignore time zones.
    message.timestamp.input.pattern=
  
    # whether timestamp field is required, it should always be required.  But
    # for historical reason, we didn't enforce this check, there might exist some
    # installations with messages missing timestamp field
    message.timestamp.required=true
  
    # To enable compression, set this to a valid compression codec implementing
    # org.apache.hadoop.io.compress.CompressionCodec interface, such as
    # 'org.apache.hadoop.io.compress.GzipCodec'.
    secor.compression.codec=org.apache.hadoop.io.compress.GzipCodec
  
    # To set a custom file extension set this to a valid file suffix, such as
    # '.gz', '.part', etc.
    secor.file.extension=.gz
  
    # The secor file reader/writer used to read/write the data, by default we write sequence files
    secor.file.reader.writer.factory=com.pinterest.secor.io.impl.DelimitedTextFileReaderWriterFactory
    #if left blank defaults to \n
    secor.file.reader.Delimiter=\n
    #if left blank no Delimiter is added. do not use \ as that needs to be escaped and is an escape
    #character and not a delimtier.
    secor.file.writer.Delimiter=\n
  
    # Max message size in bytes to retrieve via KafkaClient. This is used by ProgressMonitor and PartitionFinalizer.
    # This should be set large enough to accept the max message size configured in your kafka broker
    # Default is 0.1 MB, we set to 1MB.
    secor.max.message.size.bytes=1000000
  
    # Class that will manage uploads. Default is to use the hadoop
    # interface to S3.
    secor.upload.manager.class=com.pinterest.secor.uploader.GsUploadManager
  
    #Set below property to your timezone, and partitions in s3 will be created as per timezone provided
    secor.parser.timezone=UTC
  
    # Transformer class that transform and filters message accordingly.
    secor.message.transformer.class=com.pinterest.secor.transformer.IdentityMessageTransformer
  
    # Set below property to true if you want to have the md5hash appended to your s3 path.
    # This helps for better partitioning of the data on s3. Which gives better performance while reading and writing on s3
    secor.s3.prefix.md5hash=false
  
    # After the given date, secor will upload files to the supplied s3 alternative path
    secor.s3.alter.path.date=
  
    # An alternative S3 path for secor to upload files to
    secor.s3.alternative.path=
  
    # If enabled, add calls will be made to qubole, otherwise, skip qubole call for finalization
    secor.enable.qubole=false
  
    # Timeout value for qubole calls
    secor.qubole.timeout.ms=300000
  
    # Topics to upload at a fixed minute mark
    secor.kafka.upload_at_minute_mark.topic_filter=
  
    # What the minute mark is. This isn't triggered unless the topic name matches
    secor.upload.minute_mark=0
  
    # File age per topic and per partition is checked against secor.max.file.age.seconds by looking at
    # the youngest file when true or at the oldest file when false. Setting it to true ensures that files
    # are uploaded when data stops comming and sized based policy cannot trigger. Setting it to false
    # ensures that files older than secor.max.file.age.seconds are uploaded immediately.
    secor.file.age.youngest=true
  
    # Class that manages metric collection.
    # Sending metrics to Ostrich is the default implementation.
    secor.monitoring.metrics.collector.class=com.pinterest.secor.monitoring.OstrichMetricCollector
  
    # Row group size in bytes for Parquet writers. Specifies how much data will be buffered in memory before flushing a
    # block to disk. Larger values allow for larger column chinks which makes it possible to do larger sequential IO.
    # Should be aligned with HDFS blocks. Defaults to 128MB in Parquet 1.9.
    parquet.block.size=134217728
  
    # Page group size in bytes for Parquet writers. Indivisible unit for columnar data. Smaller data pages allow for more
    # fine grained reading but have higher space overhead. Defaults to 1MB in Parquet 1.9.
    parquet.page.size=1048576
  
    # Enable or disable dictionary encoding for Parquet writers. The dictionary encoding builds a dictionary of values
    # encountered in a given column. Defaults to true in Parquet 1.9.
    parquet.enable.dictionary=true
  
    # Enable or disable validation for Parquet writers. Validates records written against the schema. Defaults to false in
    # Parquet 1.9.
    parquet.validation=false
  
    # User can configure ORC schema for each Kafka topic. Common schema is also possible. This property is mandatory
    # if DefaultORCSchemaProvider is used. ORC schema for all the topics should be defined like this:
    secor.orc.message.schema.*=struct<a:int\,b:int\,c:struct<d:int\,e:string>\,f:array<string>\,g:int>
    # Below config used for defining ORC schema provider class name. User can use the custom implementation for orc schema provider
    secor.orc.schema.provider=com.pinterest.secor.util.orc.schema.DefaultORCSchemaProvider
  
  
  
    # Port of the Ostrich server.
    # This provide statsd statistics .. on /stats .
    ostrich.port=9998
  
    # Swift path where sequence files are stored.
    secor.swift.path=
  
  
    # Upload policies.
    # 200MB
    secor.max.file.size.bytes=200000000
    # 1 hour
    # for hourly ingestion/finalization, set this property to smaller value, e.g. 1800
    secor.max.file.age.seconds=400
  
  secor.kubernetes-test.backup.properties: |+
    include=secor.kubernetes-test.properties
  
    # Name of the Google cloud storage bucket where log files are stored.
    secor.gs.bucket=logs-secor-backup-test
  
    # Google cloud storage path where files are stored within the bucket.
    secor.gs.path=raw_logs
  
    # Name of the Kafka consumer group.
    secor.kafka.group=secor_backup
  
    # Parser class that extracts partitions from consumed messages.
    secor.message.parser.class=com.pinterest.secor.parser.OffsetMessageParser
  
    # S3 path where sequence files are stored.
    secor.s3.path=raw_logs/secor_backup
  
    # Swift path where sequence files are stored.
    secor.swift.path=
  
    # Local path where sequence files are stored before they are uploaded to s3.
    secor.local.path=/mnt/secor_data/message_logs/backup
  
    # Port of the Ostrich server.
    ostrich.port=9999
  
  secor.kubernetes-test.partition.properties: |2
  
    include=secor.kubernetes-test.properties
  
    # Name of the Google cloud storage bucket where log files are stored.
    secor.gs.bucket=logs-secor-test
  
    # Google cloud storage path where files are stored within the bucket.
    secor.gs.path=data
  
    # Name of the Kafka consumer group.
    secor.kafka.group=secor_partition
  
    # Parser class that extracts s3 partitions from consumed messages.
    secor.message.parser.class=com.pinterest.secor.parser.JsonMessageParser
  
    # Swift path where sequence files are stored.
    secor.swift.path=
  
    # Local path where sequence files are stored before they are uploaded to s3.
    secor.local.path=/mnt/secor_data/message_logs/partition
  
    # Port of the Ostrich server.
    ostrich.port=9998
  secor.kubernetes-test.properties: "# Licensed to the Apache Software Foundation (ASF)
    under one or more\n# contributor license agreements.  See the NOTICE file distributed
    with\n# this work for additional information regarding copyright ownership.\n# The
    ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\");
    you may not use this file except in compliance with\n# the License.  You may obtain
    a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#
    Unless required by applicable law or agreed to in writing, software\n# distributed
    under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR
    CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific
    language governing permissions and\n# limitations under the License.\n\n############\n#
    MUST SET #\n############\n\n# Regular expression matching names of consumed topics.\nsecor.kafka.topic_filter=log.event\nsecor.kafka.topic_blacklist=\n\n#
    Choose what to fill according to the service you are using\n# in the choice option
    you can fill S3, GS, Swift or Azure\ncloud.service=GS\n\n# AWS authentication credentials.\n#
    Leave empty if using IAM role-based authentication with s3a filesystem.\naws.access.key=\naws.secret.key=\n#
    Session token only required if using temporary S3 access keys\naws.session.token=\naws.role=\n\n#
    Optional Proxy Setting. Set to true to enable proxy\n# Only applicable to S3UploadManager\naws.proxy.isEnabled=false\naws.proxy.http.host=\naws.proxy.http.port=\n\n\nkafka.seed.broker.host=kafka-kafka.default\n\n\n################\n#
    END MUST SET #\n################\n\n\n# AWS region or endpoint. region should be
    a known region name (eg.\n# us-east-1). endpoint should be a known S3 endpoint url.
    If neither\n# are specified, then the default region (us-east-1) is used. If both\n#
    are specified then endpoint is used.\n#\n# Only apply if the the S3UploadManager
    is used - see\n# secor.upload.manager.class.\n#\n# http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region\naws.region=\naws.endpoint=\n\n#
    Toggle the AWS S3 client between virtual host style access and path style\n# access.
    See http://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html\naws.client.pathstyleaccess=false\n\n###########################\n#
    START AWS S3 ENCRYPTION #\n###########################\n\n# AWS specify type of
    server-side encryption, if any\n# set to S3 to enable S3-managed encryption\n# set
    to KMS to enable AWS KMS-managed encryption (see aws.sse.kms.key)\n# set to customer
    to enable customer-managed encryption (see aws.sse.customer.key)\n# set empty to
    disable encryption\naws.sse.type=\n\n# Key to use for S3 server-side encryption,
    base64-encoded\n# Note: requires aws.sse.type to be set to customer to be used\naws.sse.customer.key=\n\n#
    KMS Key to use for S3 server-side encryption, base64-encoded\n# Leave empty to use
    default generated key\n# Note: requires aws.sse.type to be set to  KMS to be used\naws.sse.kms.key=\n\n#########################\n#
    END AWS S3 ENCRYPTION #\n#########################\n\n# Hadoop filesystem to use.
    Choices are s3n or s3a.\n# See https://wiki.apache.org/hadoop/AmazonS3 for details.\nsecor.s3.filesystem=s3n\n\n#
    Swift config, MUST configure if cloud.service=Swift\n\n# Swift Login Details:\nswift.use.get.auth=true\nswift.auth.url=\nswift.tenant=\nswift.username=\nswift.port=8080\nswift.public=true\n\n#
    only needed if \"swift.use.get.auth\" = false\nswift.password=\n\n# only needed
    if \"swift.use.get.auth\" = true\nswift.api.key=\n\n# GS config, MUST configure
    if gcloud.service=GS\n\n# Use direct uploads\n# WARNING: disables resumable uploads,
    files are uploaded in a single request\n# This may help prevent IOException: insufficient
    data written,\n# see https://github.com/pinterest/secor/issues/177\n# https://cloud.google.com/storage/docs/json_api/v1/how-tos/upload\nsecor.gs.upload.direct=false\n
    \       \n# Limit the amonut of API calls towards GCS\n# This usually helps against
    exceptinos of: com.google.cloud.storage.StorageException: Error writing request
    body to server\n# Cold/Low traffic buckets need to be \"warmed\" up most likely
    before you can increase this value.. this is untested.\n# Sadly the client library
    doesn't say which http response code happens, ie 429 / 5xx etc.\n# Also see https://cloud.google.com/storage/docs/request-rate\nsecor.gs.tasks.ratelimit.pr.second=2\n\n#
    Old behavior of number of threads was 256 , but running this on kubernetes we experience
    we wanted less threads here,\n# to work well with ratelimit above.\nsecor.gs.threadpool.fixed.size=32\n\n#
    Application credentials configuration file\n# https://developers.google.com/identity/protocols/application-default-credentials\n#
    It can be empty when secor running in Google Cloud VMs with proper scopes\nsecor.gs.credentials.path=\n\n#
    Zookeeper config.\nzookeeper.session.timeout.ms=3000\nzookeeper.sync.time.ms=200\n\n#
    Zookeeper path (chroot) under which secor data will be placed.\nsecor.zookeeper.path=/\n\n#
    Impacts how frequently the upload logic is triggered if no messages are delivered.\nkafka.consumer.timeout.ms=10000\n\n#
    Where consumer should read from if no committed offset in zookeeper.\n#   \"smallest\"
    -> read from earliest offset\n#   \"largest\"  -> read from latest offset\n# Always
    use \"smallest\" unless you know what you're doing and are willing to risk\n# data
    loss for new topics or topics whose number of partitions has changed.\n# See the
    kafka docs for \"auto.offset.reset\".\nkafka.consumer.auto.offset.reset=smallest\n\n#
    Choose between range and roundrobin partition assignment strategy for kafka\n# high
    level consumers. Check PartitionAssignor.scala in kafa 821 module for\n# the differences
    between the two.\n# In kafka 811, only range strategy is supported.\nkafka.partition.assignment.strategy=range\n\n#
    Max number of retries during rebalance.\nkafka.rebalance.max.retries=\n\n# Rebalance
    backoff.\nkafka.rebalance.backoff.ms=\n\n# Kafka consumer receive buffer size (socket.receive.buffer.bytes)\nkafka.socket.receive.buffer.bytes=\n\n#
    Kafka fetch max size (fetch.message.max.bytes)\nkafka.fetch.message.max.bytes=\n\n#
    Kafka fetch min bytes (fetch.fetch.min.bytes)\nkafka.fetch.min.bytes=\n\n# Kafka
    fetch max wait ms (fetch.max.wait.ms)\nkafka.fetch.wait.max.ms=\n\n# Port of the
    broker serving topic partition metadata.\nkafka.seed.broker.port=9092\n\n# Zookeeper
    path at which kafka is registered. In Zookeeper parlance, this is referred\n# to
    as the chroot.\nkafka.zookeeper.path=/\n\n#URL of a Confluent Schema Registry: https://docs.confluent.io/current/schema-registry/docs/index.html\n#Only
    acquired used for decoding Avro messages\nschema.registry.url=\n\n# Store offset
    in zookeeper and kafka consumer topic.\n# Only used if kafka.offsets.storage is
    set to \"kafka\"\n# http://kafka.apache.org/documentation.html#oldconsumerconfigs\n#
    Possible values: true or false\nkafka.dual.commit.enabled=false\n\n# Storage offset.\n#
    Possible values: \"zookeeper\" to read offset from zookeeper or \"kafka\" to read
    offset from kafka consumer topic\nkafka.offsets.storage=kafka\n\n# Parameter which
    tells whether to extract Kafka message timestamp. This value is to be chose in case
    of 0.10.x kafka brokers.\n# Default value is false. Also specify `kafka.message.timestamp.className`
    as `com.pinterest.secor.timestamp.Kafka10MessageTimestamp`,\n# in case you are enabling
    this parameter as `true`.\nkafka.useTimestamp=false\n\n# Classname for the timestamp
    field you want to use. Default is `com.pinterest.secor.timestamp.Kafka10MessageTimestamp`\n#
    for 0.10 build profile. Basically, it will be `Kafka8MessageTimestamp` for 0.8 kafka
    and `Kafka10MessageTimestamp`\n# for 0.10 kafka. Fully classified names are `com.pinterest.secor.timestamp.Kafka8MessageTimestamp`
    and\n# `com.pinterest.secor.timestamp.Kafka10MessageTimestamp`.\nkafka.message.timestamp.className=com.pinterest.secor.timestamp.Kafka10MessageTimestamp\n\n#
    Classname for the message iterator you want to use. The message iterator determines
    what kind of consumer\n# secor will use to communicate with kafka. com.pinterest.secor.reader.LegacyKafkaMessageIterator
    uses\n# the old kafka consumer written scala. Its not recommended to use the legacy
    iterator with kafka version >= 1.0 since it\n# does not support the new broker protocols.
    You may face significant peformance degration on your brokers if you use it\nkafka.message.iterator.className=com.pinterest.secor.reader.LegacyKafkaMessageIterator\n\n#
    Secor generation is a version that should be incremented during non-backwards-compatible\n#
    Secor releases. Generation number is one of the components of generated log file
    names.\n# Generation number makes sure that outputs of different Secor versions
    are isolated.\nsecor.generation=1\n\n# Number of consumer threads per Secor process.\nsecor.consumer.threads=7\n\n#
    Consumption rate limit enforced at the process level (not a consumer-thread level).\nsecor.messages.per.second=10000\n\n#
    Used by the \"backup\" consumer group only.\n# Number of continuous message offsets
    that constitute a single offset= partition on s3.\n# Example:\n#   if set to 10,\n#
    \    messages with offsets 0 to 9 will be written to s3 path s3n://.../offset=0/...\n#
    \    messages with offsets 10 to 19 will be written to s3 path s3n://.../offset=10/...\n#
    \    ...\nsecor.offsets.per.partition=10000000\nsecor.offsets.prefix=offset=\n#
    How long does it take for secor to forget a topic partition. Applies to stats generation
    only.\nsecor.topic_partition.forget.seconds=600\n\n# Setting the partitioner to
    use hourly partition\n# By default, the partitioner will do daily partition, so
    the data will be\n# written into\n#       s3n://.../topic/dt=2015-07-07/\n# If this
    parameter is set to true, the data will be written into\n#       s3n://.../topic/dt=2015-07-07/hr=02\n#
    The hour folder ranges from 00 to 23\npartitioner.granularity.hour=true\npartitioner.granularity.minute=false\n\npartitioner.granularity.date.prefix=dt=\npartitioner.granularity.hour.prefix=hr=\npartitioner.granularity.minute.prefix=min=\n\npartitioner.granularity.date.format=yyyy-MM-dd\npartitioner.granularity.hour.format=HH\npartitioner.granularity.minute.format=mm\n\n#
    how many seconds should the finalizer wait to finalize a partition\npartitioner.finalizer.delay.seconds=3600\n\n#
    During partition finalization, the finalizer will start from the last\n# time partition
    (e.g. dt=2015-07-17) and traverse backwards for n\n# partition periods (e.g. dt=2015-07-16,
    dt=2015-07-15 ...)\n# This parameter controls how many partition periods to traverse
    back\n# The default is 10\n# secor.finalizer.lookback.periods=10\n\n# If greater
    than 0, upon startup Secor will clean up directories and files under secor.local.path\n#
    that are older than this value.\nsecor.local.log.delete.age.hours=1\n\n# Secor comes
    with a tool that adds Hive partitions for finalized topics. Currently, we support\n#
    only Hive clusters accessible through Qubole. The token gives access to the Qubole
    API.\n# It is available at https://api.qubole.com/users/edit\nqubole.api.token=\n\n#
    hive tables are generally named after the topics. For instance if the topic\n# is
    request_log the hive table is also called request_log. If you want this\n# to be
    pinlog_request_log you can set this config to \"pinlog_\". This affects\n# all topics.\nhive.table.prefix=\n\n#
    You can also name your hive table directly if your hive table doesn't\n# follow
    the pattern of <hive.table.prefix><kafka topic>\n# E.g.  hive.table.name.topic1=table1
    to indicate that hive table for\n# kafka topic <topic1> will be named <table1>\n\n#
    Secor can export stats such as consumption lag (in seconds and offsets) per topic
    partition.\n# Leave empty to disable this functionality.\ntsdb.hostport=\n\n# Regex
    of topics that are not exported to TSDB.\nmonitoring.blacklist.topics=\n\n# Prefix
    of exported stats.\nmonitoring.prefix=secor\n\n# Monitoring interval.\n# Set to
    0 to disable - the progress monitor will run once and exit.\nmonitoring.interval.seconds=60\n\n#
    Secor can export stats to statsd such as consumption lag (in seconds and offsets)
    per topic partition.\n# Leave empty to disable this functionality.\nstatsd.hostport=localhost:9125\n\n#
    Thrift protocol class. It applies to timestamp extractor below and parquet output
    for thrift messages.\n# TBinaryProtocol by default\nsecor.thrift.protocol.class=\n\n#
    Thrift message class. It applies to parquet output.\n# If all Kafka topics transfer
    the same thrift message type, set secor.thrift.message.class.*=<thrift class name>\nsecor.thrift.message.class.*=\n\n#
    If true, the consumer group will be the initial prefix of all\n# exported metrics,
    before `monitoring.prefix` (if set).\n#\n# Setting to false and use monitoring.prefix
    can lead to nice paths.\n# For example,\n#   secor.kafka.group = secor_hr_partition\n#
    \  monitoring.prefix = secor.hr\n#   statsd.prefixWithConsumerGroup = false\n#   =>
    secor.hr.lag.offsets.<topic>.<partition>\n#\n#   secor.kafka.group = secor_hr_partition\n#
    \  monitoring.prefix = secor\n#   statsd.prefixWithConsumerGroup = true\n#   =>
    secor_hr_partition.secor.lag.offsets.<topic>.<partition>\nstatsd.prefixWithConsumerGroup=true\n\n#
    Name of field that contains timestamp for JSON, MessagePack, or Thrift message parser.
    (1405970352123)\nmessage.timestamp.name=timestamp\n\n# Separator for defining message.timestamp.name
    in a nested structure. E.g.\n# {\"meta_data\": {\"created\": \"1405911096123\",
    \"last_modified\": \"1405912096123\"}, \"data\": \"test\"}\n# message.timestamp.name=meta_data.created\n#
    message.timestamp.name.separator=.\nmessage.timestamp.name.separator=\n\n# Field
    ID of the field that contains timestamp for Thrift message parser.\n# N.B. setting
    this past 1 will come with a performance penalty\nmessage.timestamp.id=1\n\n# Data
    type of the timestamp field for thrift message parser.\n# Supports i64 and i32.\nmessage.timestamp.type=i64\n\n#
    Name of field that contains a timestamp, as a date Format, for JSON. (2014-08-07,
    Jul 23 02:16:57 2005, etc...)\n# Should be used when there is no timestamp in a
    Long format. Also ignore time zones.\nmessage.timestamp.input.pattern=\n\n# whether
    timestamp field is required, it should always be required.  But\n# for historical
    reason, we didn't enforce this check, there might exist some\n# installations with
    messages missing timestamp field\nmessage.timestamp.required=true\n\n# To enable
    compression, set this to a valid compression codec implementing\n# org.apache.hadoop.io.compress.CompressionCodec
    interface, such as\n# 'org.apache.hadoop.io.compress.GzipCodec'.\nsecor.compression.codec=org.apache.hadoop.io.compress.GzipCodec\n\n#
    To set a custom file extension set this to a valid file suffix, such as\n# '.gz',
    '.part', etc.\nsecor.file.extension=.gz\n\n# The secor file reader/writer used to
    read/write the data, by default we write sequence files\nsecor.file.reader.writer.factory=com.pinterest.secor.io.impl.DelimitedTextFileReaderWriterFactory\n#if
    left blank defaults to \\n\nsecor.file.reader.Delimiter=\\n\n#if left blank no Delimiter
    is added. do not use \\ as that needs to be escaped and is an escape\n#character
    and not a delimtier.\nsecor.file.writer.Delimiter=\\n\n\n# Max message size in bytes
    to retrieve via KafkaClient. This is used by ProgressMonitor and PartitionFinalizer.\n#
    This should be set large enough to accept the max message size configured in your
    kafka broker\n# Default is 0.1 MB, we set to 1MB.\nsecor.max.message.size.bytes=1000000\n\n#
    Class that will manage uploads. Default is to use the hadoop\n# interface to S3.\nsecor.upload.manager.class=com.pinterest.secor.uploader.GsUploadManager\n\n#Set
    below property to your timezone, and partitions in s3 will be created as per timezone
    provided\nsecor.parser.timezone=UTC\n\n# Transformer class that transform and filters
    message accordingly.\nsecor.message.transformer.class=com.pinterest.secor.transformer.IdentityMessageTransformer\n\n#
    Set below property to true if you want to have the md5hash appended to your s3 path.\n#
    This helps for better partitioning of the data on s3. Which gives better performance
    while reading and writing on s3\nsecor.s3.prefix.md5hash=false\n\n# After the given
    date, secor will upload files to the supplied s3 alternative path\nsecor.s3.alter.path.date=\n\n#
    An alternative S3 path for secor to upload files to\nsecor.s3.alternative.path=\n\n#
    If enabled, add calls will be made to qubole, otherwise, skip qubole call for finalization\nsecor.enable.qubole=false\n\n#
    Timeout value for qubole calls\nsecor.qubole.timeout.ms=300000\n\n# Topics to upload
    at a fixed minute mark\nsecor.kafka.upload_at_minute_mark.topic_filter=\n\n# What
    the minute mark is. This isn't triggered unless the topic name matches\nsecor.upload.minute_mark=0\n\n#
    File age per topic and per partition is checked against secor.max.file.age.seconds
    by looking at\n# the youngest file when true or at the oldest file when false. Setting
    it to true ensures that files\n# are uploaded when data stops comming and sized
    based policy cannot trigger. Setting it to false\n# ensures that files older than
    secor.max.file.age.seconds are uploaded immediately.\nsecor.file.age.youngest=true\n\n#
    Class that manages metric collection.\n# Sending metrics to Ostrich is the default
    implementation.\nsecor.monitoring.metrics.collector.class=com.pinterest.secor.monitoring.OstrichMetricCollector\n\n#
    Row group size in bytes for Parquet writers. Specifies how much data will be buffered
    in memory before flushing a\n# block to disk. Larger values allow for larger column
    chinks which makes it possible to do larger sequential IO.\n# Should be aligned
    with HDFS blocks. Defaults to 128MB in Parquet 1.9.\nparquet.block.size=134217728\n\n#
    Page group size in bytes for Parquet writers. Indivisible unit for columnar data.
    Smaller data pages allow for more\n# fine grained reading but have higher space
    overhead. Defaults to 1MB in Parquet 1.9.\nparquet.page.size=1048576\n\n# Enable
    or disable dictionary encoding for Parquet writers. The dictionary encoding builds
    a dictionary of values\n# encountered in a given column. Defaults to true in Parquet
    1.9.\nparquet.enable.dictionary=true\n\n# Enable or disable validation for Parquet
    writers. Validates records written against the schema. Defaults to false in\n# Parquet
    1.9.\nparquet.validation=false\n\n# User can configure ORC schema for each Kafka
    topic. Common schema is also possible. This property is mandatory\n# if DefaultORCSchemaProvider
    is used. ORC schema for all the topics should be defined like this:\nsecor.orc.message.schema.*=struct<a:int\\,b:int\\,c:struct<d:int\\,e:string>\\,f:array<string>\\,g:int>\n#
    Below config used for defining ORC schema provider class name. User can use the
    custom implementation for orc schema provider\nsecor.orc.schema.provider=com.pinterest.secor.util.orc.schema.DefaultORCSchemaProvider\n\n\n\n#
    Port of the Ostrich server.\n# This provide statsd statistics .. on /stats .\nostrich.port=9998\n\n#
    Swift path where sequence files are stored.\nsecor.swift.path=\n\n\n# Upload policies.\n#
    200MB\nsecor.max.file.size.bytes=200000000\n# 1 hour\n# for hourly ingestion/finalization,
    set this property to smaller value, e.g. 1800\nsecor.max.file.age.seconds=60\n\n"
---
# Source: secor/templates/secor-backup-statefulset.yaml
apiVersion: apps/v1beta2
kind: StatefulSet
metadata:
  name: secor-backup
  labels:
    app: secor-backup
    chart: secor-0.1.0
    release: secor
    heritage: Helm
spec:
  serviceName: secor-backup
  selector:
    matchLabels:
      app: secor-backup
      release: secor
  updateStrategy:
    type: RollingUpdate
  replicas: 1
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: /metrics
        prometheus.io/port: "9102"
      labels:
        app: secor-backup
        release: secor

    spec:
      containers:
        - name: secor
          image: us.gcr.io/secor-demo/secor:latest
          imagePullPolicy: Always
          env:
            - name: "CONFIG_FILE"
              value: "/opt/secor/config/secor.kubernetes-dev.backup.properties"
            - name: "LOG4J_CONFIGURATION"
              value: "/opt/secor/config/log4j.docker.properties"
            - name: "GOOGLE_APPLICATION_CREDENTIALS"
              value: "/var/run/secret/cloud.google.com/service-account.json"
            - name: "ZOOKEEPER_QUORUM"
              value: "zookeeper:2181"
            - name: "ZOOKEEPER_PATH"
              value: "/"
            - name: "JVM_MEMORY"
              value: "512m"

          volumeMounts:
            - mountPath: "/opt/secor/config"
              name: "config"
            - name: "service-account"
              mountPath: "/var/run/secret/cloud.google.com"
            - name: "local-var"
              mountPath: "/mnt/secor_data/message_logs/backup"
          resources:
            requests:
              cpu: 800m
              memory: 768Mi

        - name: monitor
          image: us.gcr.io/secor-demo/secor:latest
          imagePullPolicy: Always
          env:
            - name: "SECOR_MAIN_CLASS"
              value: "com.pinterest.secor.main.ProgressMonitorMain"
            - name: "GOOGLE_APPLICATION_CREDENTIALS"
              value: "/var/run/secret/cloud.google.com/service-account.json"
            - name: "CONFIG_FILE"
              value: "/opt/secor/config/secor.kubernetes-dev.backup.properties"
            - name: "LOG4J_CONFIGURATION"
              value: "/opt/secor/config/log4j.docker-warn.properties"
            - name: "ZOOKEEPER_QUORUM"
              value: "zookeeper:2181"
            - name: "ZOOKEEPER_PATH"
              value: "/"
          volumeMounts:
            - mountPath: /opt/secor/config
              name: config
            - name: "service-account"
              mountPath: "/var/run/secret/cloud.google.com"
            - name: "local-var"
              mountPath: "/mnt/secor_data/message_logs/backup"

        - name: statsd
          image: prom/statsd-exporter:latest
          imagePullPolicy: IfNotPresent
          ports:
           - containerPort: 9102
           - containerPort: 9125
             protocol: UDP

      volumes:
        - configMap:
            name: secor-config
          name: config
        - name: "service-account"
          secret:
            secretName: secor-service-account

  volumeClaimTemplates:
    - metadata:
        name: local-var
        labels:
          app: secor-backup
        annotations:
          volume.beta.kubernetes.io/storage-class: "standard"
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "20Gi"
---
# Source: secor/templates/secor-statefulset.yaml
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: secor
  labels:
    app: secor
    chart: secor-0.1.0
    release: secor
    heritage: Helm
spec:
  serviceName: secor
  updateStrategy:
    type: RollingUpdate
  replicas: 1
  template:
    metadata:
      labels:
        app: secor
        release: secor
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9102"
        prometheus.io/path: "/metrics"
    spec:
      containers:
        - name: secor
          image: us.gcr.io/secor-demo/secor:latest
          imagePullPolicy: Always

          env:
            - name: "CONFIG_FILE"
              value: "/opt/secor/config/secor.kubernetes-dev.partition.properties"
            - name: "LOG4J_CONFIGURATION"
              value: "/opt/secor/config/log4j.docker.properties"
            - name: "GOOGLE_APPLICATION_CREDENTIALS"
              value: "/var/run/secret/cloud.google.com/service-account.json"
            - name: "ZOOKEEPER_QUORUM"
              value: "zookeeper:2181"
            - name: "ZOOKEEPER_PATH"
              value: "/"
            - name: "JVM_MEMORY"
              value: "512m"

          volumeMounts:
           - mountPath: /opt/secor/config
             name: config
           - name: "service-account"
             mountPath: "/var/run/secret/cloud.google.com"
           - name: "local-var"
             mountPath: "/mnt/secor_data/message_logs/partition"
          resources:
            requests:
              cpu: 800m
              memory: 768Mi

        - name: monitor
          image: us.gcr.io/secor-demo/secor:latest
          imagePullPolicy: Always
          env:
            - name: "CONFIG_FILE"
              value: "/opt/secor/config/secor.kubernetes-dev.partition.properties"
            - name: "LOG4J_CONFIGURATION"
              value: "/opt/secor/config/log4j.docker-warn.properties"
            - name: "SECOR_MAIN_CLASS"
              value: "com.pinterest.secor.main.ProgressMonitorMain"
            - name: "GOOGLE_APPLICATION_CREDENTIALS"
              value: "/var/run/secret/cloud.google.com/service-account.json"
            - name: "ZOOKEEPER_QUORUM"
              value: "zookeeper:2181"
            - name: "ZOOKEEPER_PATH"
              value: "/"

          volumeMounts:
            - mountPath: /opt/secor/config
              name: config
            - name: "service-account"
              mountPath: "/var/run/secret/cloud.google.com"
            - name: "local-var"
              mountPath: "/mnt/secor_data/message_logs/partition"

        - name: statsd
          image: prom/statsd-exporter:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9102
            - containerPort: 9125
              protocol: UDP

      volumes:
       - configMap:
           name: secor-config
         name: config

       - name: "service-account"
         secret:
           secretName: "secor-service-account"

  volumeClaimTemplates:
    - metadata:
        name: local-var
        labels:
          app: secor
        annotations:
          volume.beta.kubernetes.io/storage-class: "standard"
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "20Gi"
